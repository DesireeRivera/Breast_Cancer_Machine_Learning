# Eliminación de espacios en blanco múltiples
nuevo_texto <- str_replace_all(nuevo_texto,"[\\s]+", " ")
# Tokenización por palabras individuales
nuevo_texto <- str_split(nuevo_texto, " ")[[1]]
# Eliminación de tokens con una longitud < 2
nuevo_texto <- keep(.x = nuevo_texto, .p = function(x){str_length(x) > 1})
return(nuevo_texto)
}
#texto en general
texto <- MagnusCarlsen[,"full_text"]
texto <- texto %>% mutate(texto_tokenizado =map(.x= full_text, .f =limpiar_tokenizar))
#slice() para ir indexeando filas ...al hacerlo con 1, y un pull, nos enseña todos los elementos para la fila 2
texto %>% slice(1)  %>% select(texto_tokenizado)  %>% pull()
select(texto_tokenizado)
#slice() para ir indexeando filas ...al hacerlo con 1, y un pull, nos enseña todos los elementos para la fila 2
texto %>% slice(1)  %>% select(texto_tokenizado)  %>% pull()
# Ahora nuestras observaciones son palabras
texto <- cbind(usuariosTweetsMagnus$screen_name, MagnusCarlsen$id, texto)
colnames(texto) <- c("autor", "tweet_id", "texto", "texto_tokenizado")
unnest(cols= c(texto_tokenizado))
unnest(cols= c(texto$texto_tokenizado))
select(-texto)
select(texto)
#Hacemos tidy para centrarnos en frecuencias de palabras.
#Para volver de nuevo a la estructura ideal se tiene que expandir cada lista de tokens, duplicando el valor de las otras columnas tantas veces como sea necesario.
tweets_tidy <- texto %>% select(-texto) %>% unnest(cols= c(texto_tokenizado))
tweets_tidy <-  tweets_tidy %>% rename(token=texto_tokenizado)
head(tweets_tidy)
View(tweets_tidy)
tweets_tidy %>% ggplot(aes(x=autor)) + geom_bar() + coord_flip() + theme_bw()
names(head(sort(table(tweets_tidy$autor), decreasing = T)))[1:3]
head(sort(table(tweets_tidy$autor), decreasing = T))[1:3]
#Para poner los nombres que queremos
td <-  tweets_tidy[tweets_tidy$autor==names(head(sort(table(tweets_tidy$autor), decreasing = T)))[1:3],]
View(td)
View(td)
#Palabras distintas utilizadas por cada usuario
td %>% select(autor,token) %>% distinct() %>% group_by(autor) %>%
summarise(palabras_distintas=n())
sort(table(td), decreasing = T)
sort(table(td$autor), decreasing = T)
View(td)
#Con los tres que nos interesan
td%>% select(autor, token) %>% distinct() %>%
ggplot(aes(x=autor)) + geom_bar() + coord_flip() + theme_bw()
#Coord_flip es para poner las barras en horizontal. Probar comprobando que
#palabras más usadas por usuarios
td %>%group_by(autor,token)%>% count(token) %>% group_by(autor)%>%
top_n(10,n) %>% arrange( autor, desc(n)) %>% print(n=30)
#Pueden salir más de 10 palabras pero porque puede haber palabras que se repitan con la misma frecuencia que las 10 primeras
#Queremos eliminar palabras que produzcan ruido (determinantes, articulos, preposiciones)
install.packages("tm")
#install.packages("quanteda")
#library(quanteda)
library(tm)
stopwords(kind = "fr")
#Coord_flip es para poner las barras en horizontal. Probar comprobando que
#palabras más usadas por usuarios
td %>%group_by(autor,token)%>% count(token) %>% group_by(autor)%>%
top_n(10,n) %>% arrange( autor, desc(n)) %>% print(n=20)
# esta lista no es cerrada ya que es un vector de caracteres por lo que se pueden añadir palabras
lista_stopwords <- stopwords(kind = "en")
lista_stopwords <- c(lista_stopwords,c("rt","will","say","you","none","dey","whole", "least","like", "thing","much","na","take","just","de","taken"))
#Nos quedamos con todas las palabras que no esten en nuestra lista_stopwords
td <- td %>% filter(!(token %in% lista_stopwords))
td
lista_stopwords <- c(lista_stopwords,c(stopwords(kind="fr"),"rt","will","say","you","none","dey","whole", "least","like", "thing","much","na","take","just","de","taken"))
#Nos quedamos con todas las palabras que no esten en nuestra lista_stopwords
td <- td %>% filter(!(token %in% lista_stopwords))
td
View(td)
View(td)
#Gráfico
td %>% group_by(autor, token) %>% count(token) %>% group_by(autor) %>%
top_n(3, n) %>% arrange(autor, desc(n)) %>%
ggplot(aes(x = reorder(token,n), y = n, fill = autor)) +
geom_col() +
theme_bw() +
labs(y = "", x = "") +
theme(legend.position = "none") +
coord_flip() +
facet_wrap(~autor,scales = "free", ncol = 1, drop = T)
#Gráfico
td %>% group_by(autor, token) %>% count(token) %>% group_by(autor) %>%
top_n(1, n) %>% arrange(autor, desc(n)) %>%
ggplot(aes(x = reorder(token,n), y = n, fill = autor)) +
geom_col() +
theme_bw() +
labs(y = "", x = "") +
theme(legend.position = "none") +
coord_flip() +
facet_wrap(~autor,scales = "free", ncol = 1, drop = T)
#Gráfico
td %>% group_by(autor, token) %>% count(token) %>% group_by(autor) %>%
top_n(2, n) %>% arrange(autor, desc(n)) %>%
ggplot(aes(x = reorder(token,n), y = n, fill = autor)) +
geom_col() +
theme_bw() +
labs(y = "", x = "") +
theme(legend.position = "none") +
coord_flip() +
facet_wrap(~autor,scales = "free", ncol = 1, drop = T)
#Gráfico
td %>% group_by(autor, token) %>% count(token) %>% group_by(autor) %>%
top_n(3, n) %>% arrange(autor, desc(n)) %>%
ggplot(aes(x = reorder(token,n), y = n, fill = autor)) +
geom_col() +
theme_bw() +
labs(y = "", x = "") +
theme(legend.position = "none") +
coord_flip() +
facet_wrap(~autor,scales = "free", ncol = 1, drop = T)
library(tidytext)
library(syuzhet)
#Utilizamos esta matriz para cuantificar la temática de un texto, así como la importancia de cada término que lo forma.
#dfm- document frequency matrix
matriz_tfidf <- dfm(x = texto$texto, what = "word", remove_numbers = TRUE,
remove_punct = TRUE, remove_symbols = TRUE,
remove_separators = TRUE, remove_twitter = FALSE,
remove_hyphens = TRUE, remove_url = FALSE)
library(quanteda)
texto$texto_tokenizado <- as.tokens(texto$texto_tokenizado)
#Utilizamos esta matriz para cuantificar la temática de un texto, así como la importancia de cada término que lo forma.
#dfm- document frequency matrix
matriz_tfidf <- dfm(x = texto$texto, what = "word", remove_numbers = TRUE,
remove_punct = TRUE, remove_symbols = TRUE,
remove_separators = TRUE, remove_twitter = FALSE,
remove_hyphens = TRUE, remove_url = FALSE)
#Esta aproximación, aunque simple, tiene la limitación de atribuir mucha importancia a aquellas palabras que aparecen muchas veces aunque no aporten información selectiva.
matriz_tfidf <- dfm_trim(x = matriz_tfidf, min_docfreq = 15)
#Para solucionar este problema se pueden ponderar los valores tf multiplicándolos por la inversa de la frecuencia con la que el término en cuestión aparece en el resto de documentos del corpus (idf). De esta forma, se consigue reducir el valor de aquellos términos que aparecen en muchos documentos y que, por lo tanto, no aportan información selectiva.
matriz_tfidf <- dfm_tfidf(matriz_tfidf,
scheme_df = "inverse", force = T)
prueba <- as.matrix(matriz_tfidf)
head(sort(colSums(prueba), decreasing = T))
sex <- factor(c(rep("1",8),rep("2",4)),levels = c(1,2),c("Hombre","Mujer"))
levels(sex)
sex
ma <- matrix(1:1000,ncol = 10);ma
df <- data.frame(ma)
str(df)
df$X1 <- as.character(df$X1)
str(df$X1)
peso <- round(runif(1000,38,140),0)
dat <- read.csv("iris.csv")
users <- lookup_users("KremlinRussia_E, sanchezcastejon")
View(users)
View(users)
users <- lookup_users("sanchezcastejon")
View(users)
View(users)
users <- lookup_users("desiiirvr")
users <- lookup_users("desiirvr")
View(users)
View(users)
red_seguidores_putin <- get_followers("desiirvr")
View(red_seguidores_putin)
View(red_seguidores_putin)
red_amigos_putin <- get_friends("desiirvr")
View(red_amigos_putin)
View(red_amigos_putin)
pedro_s <- get_timeline("desiirvr",n = Inf, retryonratelimit = T)
View(pedro_s)
View(pedro_s)
#(2) Usuarios distintos
usuariosR <- users_data(r)
library(rtweet)
library(beepr)
#Descargar base de datos
baseR <-  search_tweets2("rihanna", n= 1500)
dfr <- data.frame(baseR)
usuariosR <- users_data(dfr);usuariosR
usuariosR <- users_data(dfr);usuariosR
#(2) Usuarios distintos
usuariosR <- users_data(dfr)
r <- readRDS("tweetsRihanna.rds")
saveRDS(baseR,"tweetsRihanna.rds")
usuariosR <- users_data(dfr);usuariosR
usuariosR <- users_data(r);usuariosR
baseRi<- dfr[,c(1,2,3,4,7,6,20,21,23,24,17,24)]
baseRi <- data.frame(baseRi)
#(2) Usuarios distintos
usuariosR <- users_data(dfr)
#(2) Usuarios distintos
usuariosR <- users_data(r)
length(levels(as.factor(usuariosR$id_str)))
length(levels(as.factor(usuariosR$name)))
proporcion <- prop.table(table(usuariosR$verified))*100
proporcion
length((usuariosR$name))
levels(as.factor(usuariosR$name))
length(levels(as.factor(usuariosR$name)))
nuevo_fluye <- usuariosR[usuariosR$favourites_count>85128.4 & usuariosR$followers_count>1773.2,]
nuevo_fluye
length(levels(as.factor(nuevo_fluye$name)))
names(table(usuarios$names))
length(levels(as.factor(usuariosR$name)))
names(table(usuarios$name))
names(table(usuariosR$name))
table(usuariosR$name)
length(table(usuariosR$name))
#Verificados
verificados <- prop.table(table(nuevo_fluye$verified))*100
verificados
length(levels(as.factor(usuarios$id_str)))
length(usuarios$id_str)
length(table(usuarios$id_str))
names(usuarios)
names(usuarios$name)
names(usuarios$name)
names(table(usuarios$name))
names(table(usuarios$name[which(usuarios$favourites_count>75653.6 & usuarios$followers_count>2118)]))
tweets <- search_tweets2("rusia & putin", n = 10000, retryonratelimit = T)
library(rtweet)
library(tidyverse)
library(knitr)
library(dplyr)
tweets <- search_tweets2("rusia & putin", n = 10000, retryonratelimit = T)
dat <- users_data(tweets)
dat <- users_data(tweets)
dat
head(tweets_tidy)
names(head(sort(table(tweets_tidy$autor), decreasing = T)))[1:3]
names(head(sort(table(tweets_tidy$autor), decreasing = T)))
sentimientos <- get_sentiment(texto$texto_tokenizado,method ="nrc", language= "english")
#Lematización: quedarnos con la raiz de la palabra y lo tomas como uno.
#Analisis de sentimiento
install.packages("tidytext")
library(tidytext)
install.packages("syuzhet")
library(syuzhet)
sentimientos <- get_sentiment(texto$texto_tokenizado,method ="nrc", language= "english")
head(sentimientos)
ggplot(dat, aes(x=sepallength))+geom_density(color="darkblue",fill="lightblue")
plot(x=dat$ALTURA, y=dat$PESO, main= "Diagrama de Dispersión",xlab= "Altura", ylab= "Peso")
users <- lookup_users("desiirvr")
View(users)
View(users)
#(3)Usuarios con mayor repercusión. 5 más influyentes de acuerdo con RT, número de seguidores
#Criterios
quantile(usuariosR$favourites_count,prob= seq(0,1,0.2))
td %>% select(autor, token) %>% distinct() %>%
ggplot(aes(x = autor)) + geom_bar() + coord_flip() + theme_bw()
td %>% select(autor, token) %>% distinct() %>%
ggplot(aes(x = autor)) + geom_bar()  + theme_bw()
td %>% select(autor, token) %>% distinct() %>%
ggplot(aes(x = autor)) + geom_bar() + coord_flip() + theme_bw("Hola")
#Graficar ese porentaje de sentimeintos
tweets_sent %>% group_by(autor, tweet_id) %>%
summarise(sentimiento_promedio = sum(valor)) %>%
group_by(autor) %>%
summarise(positivos = 100*sum(sentimiento_promedio > 0) / n(),
neutros = 100*sum(sentimiento_promedio == 0) / n(),
negativos = 100*sum(sentimiento_promedio  < 0) / n()) %>%
ungroup() %>%
gather(key = "sentimiento", value = "valor", -autor) %>%
ggplot(aes(x = autor, y = valor, fill = sentimiento)) +
geom_col(position = "dodge", color = "black") + coord_flip() +
theme_bw()
tweets_sent %>% group_by(autor, tweet_id) %>%
summarise(sentimiento_promedio = sum(valor)) %>%
group_by(autor) %>%
summarise(positivos = 100 * sum(sentimiento_promedio > 0) / n(),
neutros = 100 * sum(sentimiento_promedio == 0) / n(),
negativos = 100 * sum(sentimiento_promedio  < 0) / n())
#Se suman los sentimientos de las palabras que forman cada tweet.
tweets_sent <- tweets_tidy %>%
inner_join(sentimientos) %>%
count(texto_tokenizado, sort = TRUE)
a == b
a <- "Hola"
b <- "Hola "
#PROCESAMIENTO DE DATOS.
#Ponemos la direccion del escritorio donde esta nuestra base de datos
setwd("C:/Users/desir/OneDrive/Documentos/1.Ingenieria biomedica/3º/Mineria/Entrega_Algoritmos")
#Cargamos nuestra base de datos
dat <- read.csv("data.csv")
#EXPLORACIÓN DE DATOS
str(dat)#Vemos la estructura de los datos
#Podemos ver que la columna 33 son NA y la columna 1 es el id que no nos proporciona ninguna información por consiguiente las eliminamos
dat <- dat[,-1]
dat <- dat[,-32]
#Para una mejor visualización de las variables independientes distribuimos en las tres categorias
features_mean <- dat[,2:11]
features_se <- dat[,12:21]
features_worst <- dat[,22:31]
#Vemos los estadisticos descriptivos
summary(features_mean)
summary(features_se)
summary(features_worst)
datos_cancer_correlacion = dat[,-1]
corrplot(cor(datos_cancer_correlacion))
#Examinamos si hay outliers
library(gridExtra)
library(ggplot2)
library(ggpubr)
library(corrplot)
library(psych)
datos_cancer_correlacion = dat[,-1]
corrplot(cor(datos_cancer_correlacion))
describe(dat$diagnosis)
#Vamos a ver los datos estadísticos de nuestra variable a predecir
#Convertimos en factor la variable a predecir
dat$diagnosis <- factor(dat$diagnosis, levels= c("B","M"), labels = c("Benigno","Maligno"))
describe(dat$diagnosis)
boxplot(dat$radius_se,main= "Diagrama de Cajas Radio", col= "red")
boxplot(dat$fractal_dimension_se,main= "Diagrama de Cajas Dimension Fractal", col= "red")
boxplot(dat$radius_mean,main= "Diagrama de Cajas Radio", col= "red")
boxplot(dat$radius_se,main= "Diagrama de Cajas Radio SE", col= "red")
boxplot(dat$radius_worst,main= "Diagrama de Cajas Radio Worst", col= "red")
dat_n <- data.frame(lapply(dat[2:31] ,nom))
#Preparamos los datos con los conjuntos Train y Test
set.seed(123)
train <- sort(sample(1:nrow(dat_n),(nrow(dat_n)*70)/100))
df_train <- dat_n[train,]
dat_n <- data.frame(lapply(dat[2:31] ,nom))
#Realizamos las funciones de normalización y tipificación de los datos para medir distancias
nom <- function(x){
return((x-min(x))/(max(x)-min(x)))
}
tip <- function(x){
return(x-mean(x)/sd(x))
}
dat_n <- data.frame(lapply(dat[2:31] ,nom))
#Comprobamos que se han normalizado los datos
summary(dat_n$area_mean)
#Preparamos los datos con los conjuntos Train y Test
set.seed(123)
train <- sort(sample(1:nrow(dat_n),(nrow(dat_n)*70)/100))
df_train <- dat_n[train,]
df_test <- dat_n[-train,]
#Guardamos los resultados del diagnostico correspondiente al train y al test anteriormente realizados.
df_train_labels <- dat[train,1]
df_test_labels<- dat[-train,1]
library (class)
#Probamos con un k=21 ya que se recomienda utilizar la raiz cuadrada del numero de observaciones
dat_test_pred <- knn (train = df_train, test = df_test,
cl= df_train_labels, k=21)
head(dat_test_pred )
table(dat_test_pred)
table(df_test_labels)
confusionMatrix(df_test_labels,dat_test_pred)
#Evaluamos el modelo aplicando las medidas de rendimiento
library(caret)
confusionMatrix(df_test_labels,dat_test_pred)
precision(data = df_test_labels, reference = dat_test_pred, relevant = "Benigno")
recall(data = df_test_labels, reference = dat_test_pred, relevant = "Benigno")
F_meas(data = df_test_labels, reference = dat_test_pred, relevant = "Benigno")
dat_test_pred <- knn (train = df_train, test = df_test,
cl= df_train_labels, k=7)
confusionMatrix(df_test_labels,dat_test_pred)
precision(data = df_test_labels, reference = dat_test_pred, relevant = "Benigno")
recall(data = df_test_labels, reference = dat_test_pred, relevant = "Benigno")
F_meas(data = df_test_labels, reference = dat_test_pred, relevant = "Benigno")
#Es muy buen modelo pero vamos a intentar mejorar la precisión para ello tipificamos. Repetimos todo el ejercicio pero con los datos tipificados
dat_z <- as.data.frame(lapply(dat[2:31], tip))
dat_train <- dat_z[train, ]
dat_test <- dat_z[-train, ]
dat_test_pred <- knn(train = dat_train, test = dat_test,
cl = dat_train_labels, k=7)
precision(data = df_test_labels, reference = dat_test_pred, relevant = "Benigno")
recall(data = df_test_labels, reference = dat_test_pred, relevant = "Benigno")
F_meas(data = df_test_labels, reference = dat_test_pred, relevant = "Benigno")
confusionMatrix(dat_test_labels,dat_test_pred)
#Es muy buen modelo pero vamos a intentar mejorar la precisión para ello tipificamos. Repetimos todo el ejercicio pero con los datos tipificados
dat_z <- as.data.frame(lapply(dat[2:31], tip))
dat_train <- dat_z[train, ]
dat_test <- dat_z[-train, ]
dat_train_labels <- dat[train, 1]
dat_test_labels <- dat[-train, 1]
dat_test_pred <- knn(train = dat_train, test = dat_test,
cl = dat_train_labels, k=7)
precision(data = dat_test_labels, reference = dat_test_pred, relevant = "Benigno")
recall(data = dat_test_labels, reference = dat_test_pred, relevant = "Benigno")
F_meas(data = dat_test_labels, reference = dat_test_pred, relevant = "Benigno")
confusionMatrix(dat_test_labels,dat_test_pred)
#Para el SVM, es importante normalizar los datos para que estén todos en la misma escala.
# En este caso no es necesario transformar los datos ya que lo hará el modelo automáticamente.
# Así mismo, sólo trabaja con datos numéricos, pero no es un problema dada la BBDD disponibles.
#Preparamos nuestros datos de entrenamiento y test
set.seed(123)
train <- sort(sample(1:nrow(dat),(nrow(dat)*70)/100))
df_train <- dat[train,]
df_test <- dat[-train,]
library(kernlab)
df_classifier <- ksvm(diagnosis ~ ., data = df_train,
kernel = "vanilladot")
#Vemos su comportamiento predictor
df_predictions <- predict(df_classifier, df_test)
head(df_predictions)
confusionMatrix(df_test$diagnosis, df_predictions)
precision(data = df_test$diagnosis, reference = df_predictions, relevant = "Benigno")
recall(data = df_test$diagnosis, reference = df_predictions, relevant = "Benigno")
F_meas(data = df_test$diagnosis, reference = df_predictions, relevant = "Benigno")
#Aunque es un gran algoritmo de aprendizaje con una precisión, coeficiente Kapp y Medida F muy buenas, vamos a ver si se puede mejorar cambiando el parametro de Kernel
df_classifier_rbf <- ksvm(diagnosis ~ ., data = df_train,
kernel = "rbfdot")
df_predictions_rbf <- predict(df_classifier_rbf,
df_test)
#Aunque es un gran algoritmo de aprendizaje con una precisión, coeficiente Kapp y Medida F muy buenas, vamos a ver si se puede mejorar cambiando el parametro de Kernel
df_classifier_rbf <- ksvm(diagnosis ~ ., data = df_train,
kernel = "rbfdot")
df_predictions_rbf <- predict(df_classifier_rbf,
df_test)
confusionMatrix(df_test$diagnosis, df_predictions_rbf)
precision(data = df_test$diagnosis, reference = df_predictions_rbf, relevant = "Benigno")
recall(data = df_test$diagnosis, reference = df_predictions_rbf, relevant = "Benigno")
F_meas(data = df_test$diagnosis, reference = df_predictions_rbf, relevant = "Benigno")
#Vamos a compararlo cambiando el parametro kernel
df_classifier_t <- ksvm(diagnosis ~ ., data = df_train,
kernel = "tanhdot")
df_predictions_t <- predict(df_classifier_t,
df_test)
confusionMatrix(df_test$diagnosis, df_predictions_t)
precision(data = df_test$diagnosis, reference = df_predictions_t, relevant = "Benigno")
recall(data = df_test$diagnosis, reference = df_predictions_t, relevant = "Benigno")
F_meas(data = df_test$diagnosis, reference = df_predictions_t, relevant = "Benigno")
#Cogemos nuestra muestra para el entrenamiento y test
set.seed(123)
#Lo que hacemos es reordenar la base aleatoriamente para eliminar posibles sesgos,
#es una alternativa para dividir train y test sin sample.
dat_rand <- dat[order(runif(569)), ]
#Dividimos las muestras reordenados en 70% para el train y 30% para el test
df_train <- dat_rand[1:398, ]
df_test <- dat_rand[399:569, ]
#La función C5.0 () facilita agregar el boosting a nuestro árbol de decisión. La aplicamos a todas las columnas de la base excepto a la columna predictora
df_model <- C5.0(df_train[-1],df_train$diagnosis,rules=FALSE)
#Usamos el algoritmo de C50
library(C50)
#La función C5.0 () facilita agregar el boosting a nuestro árbol de decisión. La aplicamos a todas las columnas de la base excepto a la columna predictora
df_model <- C5.0(df_train[-1],df_train$diagnosis,rules=FALSE)
df_pred <- predict(df_model,df_test)
#Medidas de rendimiento
library(gmodels)
confusionMatrix(df_test$diagnosis, df_pred)
precision(data = df_test$diagnosis, reference = df_pred, relevant = "Benigno")
recall(data = df_test$diagnosis, reference = df_pred, relevant = "Benigno")
F_meas(data = df_test$diagnosis, reference = df_pred, relevant = "Benigno")
#Vemos que mejora respecto al anterior pero vamos a ver si se puede mejorar aun más
df_boost20 <- C5.0(df_train[-1],df_train$diagnosis,trials=20)
df_boost20
summary(df_boost20)
df_pred20 <- predict(df_boost20,df_test)
confusionMatrix(df_test$diagnosis, df_pred20)
precision(data = df_test$diagnosis, reference = df_pred20, relevant = "Benigno")
recall(data = df_test$diagnosis, reference = df_pred20, relevant = "Benigno")
F_meas(data = df_test$diagnosis, reference = df_pred20, relevant = "Benigno")
#Como podemos comprobar con 20 trials tenemos muy buen modelo por lo que nos quedamos con ese
#Dibujamos su arbol
library(dplyr)
plot(df_boost20)
##################################################################
########################### NAIVES BAYES ############################
##################################################################
#Preparamos nuestras muestras de entrenamiento y test
set.seed(123)
train <- sort(sample(1:nrow(dat),(nrow(dat)*70)/100))
#Quitamos nuestra variable a predecir
df_train <- dat[train,-1]
df_test <- dat[-train,-1]
#Incorporamos en las siguientes variable y las convertimos en factor
cod_train <- dat[train,1]
cod_test <- dat[-train,1]
cod_train <- as.factor(cod_train)
cod_test <- as.factor(cod_test)
#Comprobamos que no haya un desbalanceo de clases
prop.table(table(cod_test))
prop.table(table(cod_train))
library(e1071)
#Esta función nos pide como argumentos la variable objetivo para clasificar y los datos que serán usados.
dat_classifier <- naiveBayes(df_train, cod_train)
#Predecimos con el test
dat_test_pred <- predict(dat_classifier,df_test)
prop.table(table(dat_test_pred))
#Medidas de rendimiento
library(caret)
confusionMatrix(cod_test, dat_test_pred)
precision(data = cod_test, reference = dat_test_pred, relevant = "Benigno")
recall(data = cod_test, reference = dat_test_pred, relevant = "Benigno")
F_meas(data = cod_test, reference = dat_test_pred, relevant = "Benigno")
#Escalamos (normalizamos) y asignamos clusters aleatorios
library(stats)
#Establecemos la semilla por los k centroides aleatorios
set.seed(123)
for (i in 1:20) {
res <- kmeans(scale(dat[,-1]),i,nstart=25)
dispersion <- c(dispersion,res$tot.withinss,i)
}
#Vemos que el parametro tot.withinss es la que nos interesa para ver la dispersión y por lo tanto determinar el numero de clusters adecuado
#Buscamos cual es el k adecuado
dispersion <- NULL
for (i in 1:20) {
res <- kmeans(scale(dat[,-1]),i,nstart=25)
dispersion <- c(dispersion,res$tot.withinss,i)
}
dispersion <- as.data.frame(matrix(dispersion,ncol = 2,byrow = T))
#Usamos el metodo del codo
plot(dispersion$Grupos,dispersion$Varianza, main= "Metodo del codo")
colnames(dispersion) <- c("Varianza","Grupos")
#Usamos el metodo del codo
plot(dispersion$Grupos,dispersion$Varianza, main= "Metodo del codo")
#Vemos que no esta muy claro cuantos k coger por lo que vamos a probar con el método silhouette que es más preciso
fviz_nbclust(dat[,-1],kmeans,method="silhouette")
library("factoextra")
#Vemos que no esta muy claro cuantos k coger por lo que vamos a probar con el método silhouette que es más preciso
fviz_nbclust(dat[,-1],kmeans,method="silhouette")
res.km_5 <- kmeans(scale(dat[, -1]), 2, nstart = 25)
fviz_cluster(res.km_5, data = dat[, -1],
geom = "point",
ellipse.type = "convex",
ggtheme = theme_bw()
)
